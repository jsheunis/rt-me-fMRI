{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEUFEP-ME study: Data setup workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the workflow required to convert all raw data into the formats, types and structures necessary for the main data processing pipeline. Thus, getting data ready to be processed.\n",
    "\n",
    "## Study overview\n",
    "\n",
    "The main purpose of the study is to investigate methods to improve the quality of real-time functional magnetic resonance imaging (fMRI) data. These improvements are for future applications in real-time fMRI neurofeedback, a method where participants are presented with visual feedback of their brain activity while they are inside the MRI scanner, and then asked to regulate the level of feedback. We have developed real-time multi-echo EPI acquisition sequences and processing methods, and this study aims to collect data from volunteers in order to validate these new methods. No neurofeedback is provided.\n",
    "\n",
    "```\n",
    "The scan session will consist of a number of scan sequences, some of which will require you to look at pictures or perform a task, and others just to lie still in the scanner. The total time in the scanner will be around 1 hour, with a break in the middle. Volunteers have to be right-handed, healthy and should have no history of or be under current treatment for psychiatric / neurological conditions. If this inclusion criteria do not fit you, you will unfortunately not be able to participate in the study. Other reasons for not being able to participate include being pregnant or having metal implants.\n",
    "```\n",
    "\n",
    "## Data\n",
    "\n",
    "Per subject, all of the following data were collected during one scan session (all functional scans are multi-echo EPI):\n",
    "\n",
    "| Nr | Name  | Scan Type | Description | Format |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| 1 | T1w | Anatomical | Standard T1-weighted sequence | NIfTI |\n",
    "| 2 | run1_BOLD_rest | Functional | Resting state | PAR/REC, XML/REC, DICOM |\n",
    "| 3 | run1_BOLD_task1 | Functional | Motor - finger tapping | PAR/REC, XML/REC, DICOM |\n",
    "| 4 | run1_BOLD_task2 | Functional | Emotion - shape/face matching | PAR/REC, XML/REC, DICOM |\n",
    "| 5 | run2_BOLD_rest | Functional | Resting state | PAR/REC, XML/REC, DICOM |\n",
    "| 6 | run2_BOLD_task1 | Functional | Motor mental - imagined finger tapping | PAR/REC, XML/REC, DICOM |\n",
    "| 7 | run2_BOLD_task2 | Functional | Emotion mental - recalling emotional memories | PAR/REC, XML/REC, DICOM |\n",
    "| 8 | Stimulus timing | Peripheral measure | Stimulus and response timing for all tasks, i.e. x4 | Eprime .dat and .txt |\n",
    "| 9 | Physiology | Peripheral measure | Cardiac + respiratory traces for all runs, i.e. x6 | Philips \"scanphyslog\" |\n",
    "\n",
    "## Data setup goals\n",
    "\n",
    "For each dataset (i.e. for each subject) we have to:\n",
    "\n",
    "1. Move all files into a machine readable directory structure\n",
    "2. Rename all image files in this directory structure such that BIDS tags are findable\n",
    "3. Convert data to BIDS:\n",
    "  1. Run `bidsify` to convert the image data to BIDS (This includes conversion of PAR/REC to NIfTI using `dcm2niix`; this also includes anonymization, which doesn't work for some reason)\n",
    "  2. Deface the T1w image using `pydeface`\n",
    "  3. Run eprime conversion script to convert stimilus and response timings to BIDS (need to figure out this format)\n",
    "  4. Run `scanphyslog2bids` (or Matlab script if needed) to convert physiology data to BIDS\n",
    "4. Run the BIDS validator\n",
    "6. Create summary tables and plots using `pybids`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages / software\n",
    "\n",
    "- bidsify: https://github.com/NILAB-UvA/bidsify/tree/master/bidsify\n",
    "- pydeface: https://github.com/poldracklab/pydeface\n",
    "- fsl (required for pydeface): https://fsl.fmrib.ox.ac.uk/fsl/fslwiki\n",
    "- bids_validator: https://github.com/bids-standard/bids-validator; https://bids-standard.github.io/bids-validator/\n",
    "- convert-eprime: https://github.com/tsalo/convert-eprime\n",
    "- scanphyslog2bids: https://github.com/lukassnoek/scanphyslog2bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import bidsify\n",
    "import pydeface\n",
    "import pandas as pd\n",
    "import json\n",
    "from bids_validator import BIDSValidator\n",
    "from convert_eprime.convert import text_to_csv\n",
    "from scanphyslog2bids.core import PhilipsPhysioLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: initialize variables, directories, filenames, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent on structure and filenames in current data on external drive\n",
    "data_dir = '/Volumes/Stephan_WD/NEUFEPME_data/'\n",
    "new_data_dir = '/Volumes/Stephan_WD/NEUFEPME_data/NEUFEPME_data_new/'\n",
    "org_data_dir = '/Volumes/Stephan_WD/NEUFEPME_data_organised/'\n",
    "new_org_data_dir = org_data_dir + 'new/'\n",
    "bids_data_dir = '/Volumes/Stephan_WD/NEUFEPME_data_BIDS/'\n",
    "bids_deriv_data_dir = '/Volumes/Stephan_WD/NEUFEPME_data_BIDS_derivatives/'\n",
    "eprime_data_dir = '/Volumes/Stephan_WD/NEUFEPME_data_eprime/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1+2: Move and rename all files into a machine readable and BIDS-ready directory structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONCE ONLY: copy all raw data, with exceptions, to new directory (leave raw data untouched!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Stephan_WD/NEUFEPME_data_organised/new/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore files:\n",
    "# - with 'FSL', this is a proxy for the FSL '.nii' file\n",
    "# - with spaces (this is a proxy for the xml/rec files),\n",
    "# - the PAR/RECs of T1w file\n",
    "src = new_data_dir\n",
    "dst = new_org_data_dir\n",
    "ignore_pattern = shutil.ignore_patterns('*FSL*',\n",
    "                                        '* *',\n",
    "                                        '*T1W*.PAR',\n",
    "                                        '*T1W*.REC')\n",
    "shutil.copytree(src, dst, ignore=ignore_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONCE ONLY: Rename files such that bidsify can read separate tags and convert correctly to BIDS format\n",
    "See filenames in: \"/Users/jheunis/Documents/PYTHON/rtme-fMRI/bidsify_test_2709/sub-01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This means removing the 'SENSE' text from the T1w file such that files with SENSE are all recognised as BOLD\n",
    "os.chdir(new_org_data_dir)\n",
    "t1w = glob.glob('*/*T1W*',recursive=True)\n",
    "for fn in t1w:\n",
    "    os.rename(fn, fn.replace('_SENSE', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONCE ONLY: Rename scanphyslog files based on time of acquisition and relation to functional scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Typically 10 files in each subject's 'scanphyslog' directory. If not, throw error.\n",
    "# scan_names = ['_emotion2', '_motor2', '_rest2', '_emotion1', '_motor1', '_rest1']\n",
    "# all_subs = next(os.walk(new_org_data_dir))[1]\n",
    "\n",
    "# print(all_subs)\n",
    "# for sub in all_subs:\n",
    "#     print(sub)\n",
    "#     os.chdir(new_org_data_dir + sub + '/scanphyslog')\n",
    "#     files = glob.glob('*')\n",
    "#     files.sort(key=os.path.getmtime, reverse=True)\n",
    "#     if len(files) != 10:\n",
    "#         print('ERROR: there are {} files. Not renaming files'.format(len(files)))\n",
    "#     else:\n",
    "#         for i, file in enumerate(files):\n",
    "#             if i < 6:\n",
    "#                 new_file = os.path.splitext(file)[0] + scan_names[i] + os.path.splitext(file)[1]\n",
    "#                 print(new_file)\n",
    "#                 os.rename(file, new_file)\n",
    "#             else:\n",
    "#                 print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Convert image data to BIDS\n",
    "\n",
    "Using `bidsify`\n",
    "\n",
    "See `me_workflow.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd /Volumes/Stephan_WD/NEUFEPME_data_organised/new && bidsify -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Deface T1w images\n",
    "\n",
    "Using `pydeface`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(bids_data_dir)\n",
    "# all_subs = next(os.walk(bids_data_dir))[1]\n",
    "# print(all_subs)\n",
    "\n",
    "# for i, sub in enumerate(all_subs):\n",
    "#     anat_dir = os.path.join(bids_data_dir, sub, 'anat')\n",
    "#     T1w_fn = os.path.join(anat_dir, sub+'_T1w.nii')\n",
    "#     T1w_defaced_fn = os.path.join(anat_dir, sub+'_T1w_defaced.nii')\n",
    "#     if not os.path.exists(T1w_defaced_fn):\n",
    "#         print('{} - {}'.format(i, T1w_fn))\n",
    "#         #!cd $anat_dir && pydeface $T1w_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: Convert Eprime data to BIDS\n",
    "\n",
    "Use `neufepme_read_eprime.ipynb`, which uses `convert-eprime`\n",
    "\n",
    "see: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/05-task-events.html\n",
    "\n",
    "Perhaps write this into separate functions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utilities as util\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "# paradigms = ['motor-', 'motor-imagine-', 'emotion-', 'emotion-imagine-']\n",
    "# paradigm_nrs = ['-001', '-002', '-001', '-002']\n",
    "# event_names = ['_task-motor_run-1_', '_task-motor_run-2_', '_task-emotion_run-1_', '_task-emotion_run-2_']\n",
    "# keys = ['df0', 'df1', 'df2', 'df3']\n",
    "# dct_df = dict.fromkeys(keys, None)\n",
    "\n",
    "# # Loop through all subjects in BIDS folder\n",
    "# os.chdir(bids_data_dir)\n",
    "# all_subs = next(os.walk(bids_data_dir))[1]\n",
    "\n",
    "# for sub in all_subs:\n",
    "#     # Find text files for each subject's four runs (see `paradigms` for run types)\n",
    "#     # Create empty csv files (into which text file content is copied) and tsv files (for events in BIDS format)\n",
    "#     sub_data_dir = os.path.join(eprime_data_dir, sub)\n",
    "#     print(sub_data_dir)\n",
    "#     txt_files = [None] * 4\n",
    "#     csv_files = [None] * 4\n",
    "#     event_files = [None] * 4\n",
    "#     sub_nr = sub.split('-')[1]\n",
    "#     for i, par in enumerate(paradigms):\n",
    "#         txt_files[i] = os.path.join(sub_data_dir, 'neufepme-' + par + sub_nr + paradigm_nrs[i] + '.txt')\n",
    "#         name, ext = os.path.splitext(txt_files[i])\n",
    "#         csv_files[i] = name + '.csv'\n",
    "#         event_files[i] = os.path.join(bids_data_dir, sub, 'func', sub+event_names[i]+'events.tsv')\n",
    "#     # Read text files, convert to csv files, and cv files into dataframes \n",
    "#     all_dfs = {}\n",
    "#     for i, fn in enumerate(txt_files):\n",
    "#         text_to_csv(fn, csv_files[i])\n",
    "#         df = pd.read_csv(csv_files[i])\n",
    "#         all_dfs[paradigms[i]] = df\n",
    "\n",
    "#     dct_df = util.create_event_files(all_dfs, paradigms, dct_df, sub)\n",
    "\n",
    "\n",
    "#     ## FIX ALL THE DATAFRAMES TO MAKE THEM BIDS COMPATIBLE\n",
    "#     # Drop the 'Rest' rows from df0, df1, df3\n",
    "#     if not sub == 'sub-001':\n",
    "#         for i in [0,1,3]:\n",
    "#             df = dct_df[keys[i]]\n",
    "#             idx = df[df['trial_type'] == 'Rest'].index\n",
    "#             df.drop(idx , inplace=True)\n",
    "\n",
    "#     # Update df2:\n",
    "#     df = dct_df['df2']\n",
    "#     # fill in missing values for sub-001:\n",
    "#     if sub == 'sub-001':\n",
    "#         df.loc[60, 'duration'] = 3.012\n",
    "#         df.loc[60, 'response_time'] = 0.000\n",
    "#     # 'button_pressed' = NaN ==> n/a; same rows 'response_time' ==> n/a\n",
    "#     nan_rows = df[df['button_pressed'].isnull()]\n",
    "#     df.loc[nan_rows.index, 'response_time'] = 'n/a'\n",
    "#     df.loc[nan_rows.index, 'button_pressed'] = 'n/a'\n",
    "#     # r = red = RIGHT ==> R; g = green = LEFT ==> L\n",
    "#     r_rows = df[df['button_pressed'] == 'r']\n",
    "#     df.loc[r_rows.index, 'button_pressed'] = 'R'\n",
    "#     g_rows = df[df['button_pressed'] == 'g']\n",
    "#     df.loc[g_rows.index, 'button_pressed'] = 'L'\n",
    "#     # Fix onset times\n",
    "#     df.loc[0, 'onset'] = 0.000\n",
    "#     for i in df.index:\n",
    "#         if i > 0:\n",
    "#             df.loc[i, 'onset'] = np.round(df.loc[i-1, 'onset'] + df.loc[i-1, 'duration'], 3)\n",
    "\n",
    "#     print(dct_df)\n",
    "\n",
    "#     # Write to TSV files\n",
    "#     for i, key in enumerate(keys):\n",
    "#         if sub == 'sub-001':\n",
    "#             if i==2:\n",
    "#                 events = dct_df[key]\n",
    "#                 events.to_csv(event_files[i], sep='\\t', index=False)\n",
    "#         else:   \n",
    "#             events = dct_df[key]\n",
    "#             events.to_csv(event_files[i], sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.4: Convert physiology data to BIDS\n",
    "\n",
    "Using `scanphyslog2bids`\n",
    "\n",
    "See: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/06-physiological-and-other-continuous-recordings.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Stephan_WD/NEUFEPME_data_BIDS_derivatives/scanphyslog2bids/sub-002\n",
      "/Volumes/Stephan_WD/NEUFEPME_data_organised/sub-002/scanphyslog\n",
      "/Volumes/Stephan_WD/NEUFEPME_data_organised/sub-002/scanphyslog/sub-002_task-rest_run-1_physio.log\n",
      "/Volumes/Stephan_WD/NEUFEPME_data_BIDS/sub-002/func\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 15:21:23,786 [MainThread] [INFO   ]  Found end marker ('0020') at an offset of 16 samples relative to the end of the file.\n",
      "2020-01-30 15:21:27,642 [MainThread] [INFO   ]  Trimmed off 62 samples from end of file based on the (absence) of a gradient.\n",
      "2020-01-30 15:21:27,644 [MainThread] [INFO   ]  Found 210 triggers with a mean duration of 2.00200 (0.00000)!\n",
      "2020-01-30 15:21:28,501 [MainThread] [INFO   ]  Saving BIDS data to /Volumes/Stephan_WD/NEUFEPME_data_BIDS/sub-002/func/sub-002_task-rest_run-1_physio.tsv.\n",
      "/Users/jheunis/miniconda3/envs/rtme-fmri/lib/python3.7/site-packages/scanphyslog2bids/core.py:570: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.tight_layout()\n",
      "/Users/jheunis/miniconda3/envs/rtme-fmri/lib/python3.7/site-packages/scanphyslog2bids/core.py:581: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.savefig(base_name, dpi=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Stephan_WD/NEUFEPME_data_organised/sub-002/scanphyslog/sub-002_task-motor_run-1_physio.log\n",
      "/Volumes/Stephan_WD/NEUFEPME_data_BIDS/sub-002/func\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 15:21:34,555 [MainThread] [INFO   ]  Found end marker ('0020') at an offset of 73 samples relative to the end of the file.\n",
      "2020-01-30 15:21:39,193 [MainThread] [INFO   ]  Trimmed off 61 samples from end of file based on the (absence) of a gradient.\n",
      "2020-01-30 15:21:39,195 [MainThread] [INFO   ]  Found 210 triggers with a mean duration of 2.00200 (0.00000)!\n",
      "2020-01-30 15:21:40,239 [MainThread] [INFO   ]  Saving BIDS data to /Volumes/Stephan_WD/NEUFEPME_data_BIDS/sub-002/func/sub-002_task-motor_run-1_physio.tsv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Stephan_WD/NEUFEPME_data_organised/sub-002/scanphyslog/sub-002_task-emotion_run-1_physio.log\n",
      "/Volumes/Stephan_WD/NEUFEPME_data_BIDS/sub-002/func\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 15:21:46,233 [MainThread] [INFO   ]  Found end marker ('0020') at an offset of 38 samples relative to the end of the file.\n",
      "2020-01-30 15:21:50,960 [MainThread] [INFO   ]  Trimmed off 58 samples from end of file based on the (absence) of a gradient.\n",
      "2020-01-30 15:21:50,961 [MainThread] [INFO   ]  Found 210 triggers with a mean duration of 2.00200 (0.00000)!\n",
      "2020-01-30 15:21:51,929 [MainThread] [INFO   ]  Saving BIDS data to /Volumes/Stephan_WD/NEUFEPME_data_BIDS/sub-002/func/sub-002_task-emotion_run-1_physio.tsv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Stephan_WD/NEUFEPME_data_organised/sub-002/scanphyslog/sub-002_task-rest_run-2_physio.log\n",
      "/Volumes/Stephan_WD/NEUFEPME_data_BIDS/sub-002/func\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 15:21:58,286 [MainThread] [INFO   ]  Found end marker ('0020') at an offset of 77 samples relative to the end of the file.\n",
      "2020-01-30 15:22:02,557 [MainThread] [INFO   ]  Trimmed off 59 samples from end of file based on the (absence) of a gradient.\n",
      "2020-01-30 15:22:02,558 [MainThread] [INFO   ]  Found 210 triggers with a mean duration of 2.00200 (0.00000)!\n",
      "2020-01-30 15:22:03,390 [MainThread] [INFO   ]  Saving BIDS data to /Volumes/Stephan_WD/NEUFEPME_data_BIDS/sub-002/func/sub-002_task-rest_run-2_physio.tsv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Stephan_WD/NEUFEPME_data_organised/sub-002/scanphyslog/sub-002_task-motor_run-2_physio.log\n",
      "/Volumes/Stephan_WD/NEUFEPME_data_BIDS/sub-002/func\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 15:22:09,377 [MainThread] [INFO   ]  Found end marker ('0020') at an offset of 92 samples relative to the end of the file.\n",
      "2020-01-30 15:22:13,846 [MainThread] [INFO   ]  Trimmed off 60 samples from end of file based on the (absence) of a gradient.\n",
      "2020-01-30 15:22:13,847 [MainThread] [INFO   ]  Found 210 triggers with a mean duration of 2.00200 (0.00000)!\n",
      "2020-01-30 15:22:14,724 [MainThread] [INFO   ]  Saving BIDS data to /Volumes/Stephan_WD/NEUFEPME_data_BIDS/sub-002/func/sub-002_task-motor_run-2_physio.tsv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Stephan_WD/NEUFEPME_data_organised/sub-002/scanphyslog/sub-002_task-emotion_run-2_physio.log\n",
      "/Volumes/Stephan_WD/NEUFEPME_data_BIDS/sub-002/func\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 15:22:20,795 [MainThread] [INFO   ]  Found end marker ('0020') at an offset of 81 samples relative to the end of the file.\n",
      "2020-01-30 15:22:25,299 [MainThread] [INFO   ]  Trimmed off 62 samples from end of file based on the (absence) of a gradient.\n",
      "2020-01-30 15:22:25,300 [MainThread] [INFO   ]  Found 210 triggers with a mean duration of 2.00200 (0.00000)!\n",
      "2020-01-30 15:22:26,181 [MainThread] [INFO   ]  Saving BIDS data to /Volumes/Stephan_WD/NEUFEPME_data_BIDS/sub-002/func/sub-002_task-emotion_run-2_physio.tsv.\n"
     ]
    }
   ],
   "source": [
    "# Create folder into which scanphyslog2bids image outputs will be saved\n",
    "bids_physio_dir = os.path.join(bids_deriv_data_dir, 'scanphyslog2bids')\n",
    "if not os.path.exists(bids_physio_dir):\n",
    "    os.makedirs(bids_physio_dir)\n",
    "\n",
    "tasks = ['rest1', 'motor1', 'emotion1', 'rest2', 'motor2', 'emotion2']\n",
    "tasks_bids = ['rest_run-1', 'motor_run-1', 'emotion_run-1', 'rest_run-2', 'motor_run-2', 'emotion_run-2']\n",
    "\n",
    "os.chdir(bids_data_dir)\n",
    "all_subs = next(os.walk(bids_data_dir))[1]\n",
    "\n",
    "for sub in all_subs:\n",
    "    if sub == 'sub-002':\n",
    "        # Create folder into which scanphyslog2bids image outputs will be saved, for current subject\n",
    "        sub_physio_dir = os.path.join(bids_physio_dir, sub)\n",
    "        if not os.path.exists(sub_physio_dir):\n",
    "            os.makedirs(sub_physio_dir)\n",
    "        print(sub_physio_dir)\n",
    "        # Initialise variables\n",
    "        log_fns = [None] * len(tasks)\n",
    "        log_renamed_fns = [None] * len(tasks)\n",
    "        bids_fns = [None] * len(tasks)\n",
    "        # Navigate to raw scanphyslog data for current subject\n",
    "        scanphys_data_dir = os.path.join(org_data_dir, sub, 'scanphyslog')\n",
    "        print(scanphys_data_dir)\n",
    "        os.chdir(scanphys_data_dir)\n",
    "        # For each scanphyslog file related to a specific functional run:\n",
    "        for i, task in enumerate(tasks):\n",
    "            fn = glob.glob('*'+task+'*')[0]\n",
    "            log_fns[i] = os.path.join(scanphys_data_dir, fn)\n",
    "            log_renamed_fns[i] = os.path.join(scanphys_data_dir, sub+'_task-'+tasks_bids[i]+'_physio.log')\n",
    "            print(log_renamed_fns[i])\n",
    "            # Copy and rename scanphyslog file to BIDS-compatible naming structure\n",
    "            shutil.copy(log_fns[i], log_renamed_fns[i])\n",
    "            # File to be converted\n",
    "            log_file = log_renamed_fns[i]\n",
    "            # Folder where the BIDSified data should be saved\n",
    "            out_dir = os.path.join(bids_data_dir, sub, 'func')\n",
    "            print(out_dir)\n",
    "            # Folder where some QC plots should be saved\n",
    "            deriv_dir = sub_physio_dir\n",
    "            # fmri_file is used to extract metadata, such as TR and number of volumes\n",
    "            fmri_file = os.path.join(out_dir, sub+'_task-'+tasks_bids[i]+'_echo-2_bold.nii' ) \n",
    "            fmri_img = nib.load(fmri_file)\n",
    "            n_dyns = fmri_img.shape[-1]\n",
    "            tr = np.round(fmri_img.header['pixdim'][4], 3)\n",
    "            # Create PhilipsPhysioLog object with info\n",
    "            phlog = PhilipsPhysioLog(f=log_file, tr=tr, n_dyns=n_dyns, sf=500, manually_stopped=False)\n",
    "            # Load in data, do some preprocessing\n",
    "            phlog.load()\n",
    "            # Try to align physio data with scan data, using a particular method\n",
    "            # (either \"vol_markers\", \"gradient_log\", or \"interpolate\")\n",
    "            phlog.align(trigger_method='interpolate')  # load and find vol triggers\n",
    "            # Write out BIDS files - .tsv.gz and .json files\n",
    "            phlog.to_bids(out_dir)\n",
    "            # Optional: plot some QC graphs for alignment and actual traces\n",
    "            phlog.plot_alignment(out_dir=deriv_dir)  # plots alignment with gradient\n",
    "            phlog.plot_traces(out_dir=deriv_dir)  # plots cardiac/resp traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5: Add slice-timing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First run scripts to ensure that BIDS json data is correct\n",
    "# TR = 2\n",
    "# N_slices = 34\n",
    "# t_d = TR/N_slices\n",
    "# t_start = 0\n",
    "# t_stop = TR\n",
    "# slice_timing = np.arange(start=t_start, stop=t_stop, step=t_d).round(4).tolist()\n",
    "\n",
    "# os.chdir(bids_data_dir)\n",
    "# all_subs = next(os.walk(bids_data_dir))[1]\n",
    "# for sub in all_subs:\n",
    "#     os.chdir(os.path.join(bids_data_dir, sub, 'func'))\n",
    "#     json_files = glob.glob('*bold.json')\n",
    "#     if len(json_files) != 18:\n",
    "#         print('ERROR: there are {} BOLD json files, while there should be 18. Skipping subject: {}'.format(len(json_files), sub))\n",
    "#     else:\n",
    "#         for json_fn in json_files:\n",
    "#             with open(json_fn) as json_file:\n",
    "#                 json_decoded = json.load(json_file)\n",
    "\n",
    "#             json_decoded[\"SliceTiming\"] = slice_timing\n",
    "\n",
    "#             with open(json_fn, 'w') as json_file:\n",
    "#                 json.dump(json_decoded, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.6 cleanup json bids files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run BIDS validator\n",
    "\n",
    "see: https://bids-standard.github.io/bids-validator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if possible to run validator on a full directory tree\n",
    "\n",
    "# fn = '/Users/jheunis/Documents/PYTHON/rtme-fMRI/bidsify_test_2709/sub-01/anat/sub-01_T1w.nii'\n",
    "# BIDSValidator().is_bids(fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create summary tables and plots using `pybids`\n",
    "\n",
    "See:\n",
    "- https://bids-standard.github.io/pybids/\n",
    "- https://github.com/bids-standard/pybids/blob/master/examples/pybids_tutorial.ipynb\n",
    "- https://mybinder.org/v2/gh/bids-standard/pybids/master\n",
    "\n",
    "this is also useful for de/constructing filenames, might be useful in earlier steps of this workflow already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Data processing notes\n",
    "\n",
    "Pybids gave `unknown locale UTF-8` issue, fixed by adding values to path variable upon conda env startup (and removing upon deactivation), see: https://coderwall.com/p/-k_93g/mac-os-x-valueerror-unknown-locale-utf-8-in-python\n",
    "\n",
    "Need to build eprime code into a script, currently in a notebook\n",
    "\n",
    "Remember for BIDS-validator:\n",
    "- Delete T1w file and rename defaced file to original T1w name\n",
    "- add events.tsv\n",
    "- SliceTiming required for all functionals - need to find a better place to implement this in future. For now just a script to run through all files and update json with precalculated slice times based on known parameters. This is needed since Philips does not supply slice timing information in PAR/rec or DICOM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data quality notes\n",
    "\n",
    "#### Problem subjects: Eprime\n",
    "\n",
    "- **sub-001** - Settings were not correct for saving OnsetToOnset time, Response time, Response, and other values. This means that missing data had to be completed. Steps done:\n",
    "    - a) For event files for motor-run-1, motor-run-2 and emotion-run-2: copied event files from other subjects since the block paradigm stimulus timings are equal (to 2 decimal places).\n",
    "    - b) For event file for emotion-run-2, only some values were missing, had to add some NaNs and replace this with average timing values from other stimulus timings from same run, same subject.\n",
    "\n",
    "\n",
    "#### Problem subjects: EPI acquisition\n",
    "\n",
    "- **sub-008** - last scan not finished (recon issue), need to rescan\n",
    "- **sub-014** - last scan not finished (recon issue), need to rescan\n",
    "\n",
    "#### Problem subjects: scanphyslog\n",
    "\n",
    "UPDATE 13 January 2020:\n",
    "*I incorrectly attributed the emotion1 scan restart problem to sub-015, it was actually sub-016 as confirmed on RIS system (see photos). This also explains \"extra small file\" previously mentioned in sub-016 dataset*\n",
    "\n",
    "- **sub-015** - emotion1 scan was started twice, because heartrate sensor was not correctly recording. Second time was fine. need to inspect earlier scans for same issue. also, looks like scanphyslog didn't restart and kept loggin to same file, resulting in >12mb log file. need to inspect\n",
    "- **sub-016** - All image data fine. Have to look at this data on RIS since scanphyslog has one extra small file which doesn't seem to fit\n",
    "\n",
    "#### Problem subjects: T1w display\n",
    "\n",
    "- **sub-017** - when displaying in Mango, top slices of brain are cut off, need to check if this is a viewer issue or acquisition issue. Need to check other subjects as well.\n",
    "\n",
    "#### Steps done for now (8 Jan 2020):\n",
    "\n",
    "- **sub-008** - deleted from organised data and BIDS data\n",
    "- **sub-014** - deleted from organised data and BIDS data\n",
    "- **sub-015** - removed from BIDS data, kept in organised data separate folder, scanphyslog renamed, need to inspect\n",
    "- **sub-016** - removed from BIDS data, kept in organised data separate folder, scanphyslog NOT renamed, need to inspect\n",
    "\n",
    "#### Steps to do (UPDATE 13 Jan 2020):\n",
    "\n",
    "- **sub-015** - move back to BIDS and ensure everything is correctly named and converted\n",
    "- **sub-016** - inspect everything, ensure this is correctly named and converted (ignore small file).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
