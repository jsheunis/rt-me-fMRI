{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEUFEP-ME study: Data setup workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the workflow required to convert all raw data into the formats, types and structures necessary for the main data processing pipeline. Thus, getting data ready to be processed.\n",
    "\n",
    "## Study overview\n",
    "\n",
    "The main purpose of the study is to investigate methods to improve the quality of real-time functional magnetic resonance imaging (fMRI) data. These improvements are for future applications in real-time fMRI neurofeedback, a method where participants are presented with visual feedback of their brain activity while they are inside the MRI scanner, and then asked to regulate the level of feedback. We have developed real-time multi-echo EPI acquisition sequences and processing methods, and this study aims to collect data from volunteers in order to validate these new methods. No neurofeedback is provided.\n",
    "\n",
    "```\n",
    "The scan session will consist of a number of scan sequences, some of which will require you to look at pictures or perform a task, and others just to lie still in the scanner. The total time in the scanner will be around 1 hour, with a break in the middle. Volunteers have to be right-handed, healthy and should have no history of or be under current treatment for psychiatric / neurological conditions. If this inclusion criteria do not fit you, you will unfortunately not be able to participate in the study. Other reasons for not being able to participate include being pregnant or having metal implants.\n",
    "```\n",
    "\n",
    "## Data\n",
    "\n",
    "Per subject, all of the following data were collected during one scan session (all functional scans are multi-echo EPI):\n",
    "\n",
    "| Nr | Name  | Scan Type | Description | Format |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| 1 | T1w | Anatomical | Standard T1-weighted sequence | NIfTI |\n",
    "| 2 | run1_BOLD_rest | Functional | Resting state | PAR/REC, XML/REC, DICOM |\n",
    "| 3 | run1_BOLD_task1 | Functional | Motor - finger tapping | PAR/REC, XML/REC, DICOM |\n",
    "| 4 | run1_BOLD_task2 | Functional | Emotion - shape/face matching | PAR/REC, XML/REC, DICOM |\n",
    "| 5 | run2_BOLD_rest | Functional | Resting state | PAR/REC, XML/REC, DICOM |\n",
    "| 6 | run2_BOLD_task1 | Functional | Motor mental - imagined finger tapping | PAR/REC, XML/REC, DICOM |\n",
    "| 7 | run2_BOLD_task2 | Functional | Emotion mental - recalling emotional memories | PAR/REC, XML/REC, DICOM |\n",
    "| 8 | Stimulus timing | Peripheral measure | Stimulus and response timing for all tasks, i.e. x4 | Eprime .dat and .txt |\n",
    "| 9 | Physiology | Peripheral measure | Cardiac + respiratory traces for all runs, i.e. x6 | Philips \"scanphyslog\" |\n",
    "\n",
    "## Data setup goals\n",
    "\n",
    "For each dataset (i.e. for each subject) we have to:\n",
    "\n",
    "1. Move all files into a machine readable directory structure\n",
    "2. Rename all image files in this directory structure such that BIDS tags are findable\n",
    "3. Convert data to BIDS:\n",
    "  1. Run `bidsify` to convert the image data to BIDS (This includes conversion of PAR/REC to NIfTI using `dcm2niix`; this also includes anonymization, which doesn't work for some reason)\n",
    "  2. Deface the T1w image using `pydeface`\n",
    "  3. Run eprime conversion script to convert stimilus and response timings to BIDS (need to figure out this format)\n",
    "  4. Run `scanphyslog2bids` (or Matlab script if needed) to convert physiology data to BIDS\n",
    "4. Run the BIDS validator\n",
    "6. Create summary tables and plots using `pybids`\n",
    "\n",
    "## Data quality notes\n",
    "\n",
    "#### Problem subjects: EPI acquisition\n",
    "\n",
    "- **sub-008** - last scan not finished (recon issue), need to rescan\n",
    "- **sub-014** - last scan not finished (recon issue), need to rescan\n",
    "\n",
    "#### Problem subjects: scanphyslog\n",
    "\n",
    "UPDATE 13 January 2020:\n",
    "*I incorrectly attributed the emotion1 scan restart problem to sub-015, it was actually sub-016 as confirmed on RIS system (see photos). This also explains \"extra small file\" previously mentioned in sub-016 dataset*\n",
    "\n",
    "- **sub-015** - emotion1 scan was started twice, because heartrate sensor was not correctly recording. Second time was fine. need to inspect earlier scans for same issue. also, looks like scanphyslog didn't restart and kept loggin to same file, resulting in >12mb log file. need to inspect\n",
    "- **sub-016** - All image data fine. Have to look at this data on RIS since scanphyslog has one extra small file which doesn't seem to fit\n",
    "\n",
    "#### Problem subjects: T1w display\n",
    "\n",
    "- **sub-017** - when displaying in Mango, top slices of brain are cut off, need to check if this is a viewer issue or acquisition issue. Need to check other subjects as well.\n",
    "\n",
    "#### Steps done for now (8 Jan 2020):\n",
    "\n",
    "- **sub-008** - deleted from organised data\n",
    "- **sub-014** - deleted from organised data\n",
    "- **sub-015** - removed from BIDS data, kept in organised data separate folder, scanphyslog renamed, need to inspect\n",
    "- **sub-016** - removed from BIDS data, kept in organised data separate folder, scanphyslog NOT renamed, need to inspect\n",
    "\n",
    "#### Steps to do (UPDATE 13 Jan 2020):\n",
    "\n",
    "- **sub-015** - move back to BIDS and ensure everything is correctly named and converted\n",
    "- **sub-016** - inspect everything, ensure this is correctly named and converted (ignore small file).\n",
    "\n",
    "\n",
    "## Data processing notes\n",
    "\n",
    "Pybids gave `unknown locale UTF-8` issue, fixed by adding values to path variable upon conda env startup (and removing upon deactivation), see: https://coderwall.com/p/-k_93g/mac-os-x-valueerror-unknown-locale-utf-8-in-python\n",
    "\n",
    "Need to build eprime code into a script, currently in a notebook\n",
    "\n",
    "Remember for BIDS-validator:\n",
    "- Delete T1w file and rename defaced file to original T1w name\n",
    "- add events.tsv\n",
    "- SliceTiming required for all functionals - need to find a better place to implement this in future. For now just a script to run through all files and update json with precalculated slice times based on known parameters. This is needed since Philips does not supply slice timing information in PAR/rec or DICOM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages / software\n",
    "\n",
    "- bidsify: https://github.com/NILAB-UvA/bidsify/tree/master/bidsify\n",
    "- pydeface: https://github.com/poldracklab/pydeface\n",
    "- fsl (required for pydeface): https://fsl.fmrib.ox.ac.uk/fsl/fslwiki\n",
    "- bids_validator: https://github.com/bids-standard/bids-validator; https://bids-standard.github.io/bids-validator/\n",
    "- convert-eprime: https://github.com/tsalo/convert-eprime\n",
    "- scanphyslog2bids: https://github.com/lukassnoek/scanphyslog2bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import bidsify\n",
    "import pydeface\n",
    "# import nipype\n",
    "from bids_validator import BIDSValidator\n",
    "from convert_eprime.utils import remove_unicode\n",
    "from convert_eprime.convert import text_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: initialize variables, directories, filenames, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent on structure and filenames in current data on external drive\n",
    "\n",
    "data_dir = '/Volumes/Stephan_WD/NEUFEPME_data/'\n",
    "org_data_dir = '/Volumes/Stephan_WD/NEUFEPME_data_organised/'\n",
    "bids_data_dir = '/Volumes/Stephan_WD/NEUFEPME_data_BIDS/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1+2: Move and rename all files into a machine readable and BIDS-ready directory structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONCE ONLY: copy all raw data, with exceptions, to new directory (leave raw data untouched!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ignore files:\n",
    "# # - with 'FSL', this is a proxy for the FSL '.nii' file\n",
    "# # - with spaces (this is a proxy for the xml/rec files),\n",
    "# # - the PAR/RECs of T1w file\n",
    "# src = data_dir\n",
    "# dst = org_data_dir\n",
    "# ignore_pattern = shutil.ignore_patterns('*FSL*',\n",
    "#                                         '* *',\n",
    "#                                         '*T1W*.PAR',\n",
    "#                                         '*T1W*.REC')\n",
    "# shutil.copytree(src, dst, ignore=ignore_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONCE ONLY: Rename files such that bidsify can read separate tags and convert correctly to BIDS format\n",
    "See filenames in: \"/Users/jheunis/Documents/PYTHON/rtme-fMRI/bidsify_test_2709/sub-01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This means removing the 'SENSE' text from the T1w file such that files with SENSE are all recognised as BOLD\n",
    "# os.chdir(org_data_dir)\n",
    "# t1w = glob.glob('*/*T1W*',recursive=True)\n",
    "# for fn in t1w:\n",
    "#     os.rename(fn, fn.replace('_SENSE', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONCE ONLY: Rename scanphyslog files based on time of acquisition and relation to functional scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically 10 files in each subject's 'scanphyslog' directory. If not, throw error.\n",
    "# scan_names = ['_emotion2', '_motor2', '_rest2', '_emotion1', '_motor1', '_rest1']\n",
    "# all_subs = next(os.walk(org_data_dir))[1]\n",
    "# for sub in all_subs:\n",
    "#     print(sub)\n",
    "#     os.chdir(org_data_dir + sub + '/scanphyslog')\n",
    "#     files = glob.glob('*')\n",
    "#     files.sort(key=os.path.getmtime, reverse=True)\n",
    "#     if len(files) != 10:\n",
    "#         print('ERROR: there are {} files. Not renaming files'.format(len(files)))\n",
    "#     else:\n",
    "#         for i, file in enumerate(files):\n",
    "#             if i < 6:\n",
    "#                 new_file = os.path.splitext(file)[0] + scan_names[i] + os.path.splitext(file)[1]\n",
    "#                 print(new_file)\n",
    "#                 os.rename(file, new_file)\n",
    "#             else:\n",
    "#                 print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Convert image data to BIDS\n",
    "\n",
    "Using `bidsify`\n",
    "\n",
    "See `me_workflow.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd /Volumes/Stephan_WD/NEUFEPME_data_organised && bidsify -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Deface T1w images\n",
    "\n",
    "Using `pydeface`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200109-13:54:29,22 nipype.utils INFO:\n",
      "\t Running nipype version 1.2.3 (latest: 1.4.0)\n",
      "------------\n",
      "pydeface 2.0\n",
      "------------\n",
      "Temporary files:\n",
      "  /var/folders/v1/00bvrxc97sndvb45mvrzd_2r0000gn/T/tmpgn7_a64b.mat\n",
      "  /var/folders/v1/00bvrxc97sndvb45mvrzd_2r0000gn/T/tmpl4u6b6lc.nii.gz\n",
      "Defacing...\n",
      "  sub-017_T1w.nii\n",
      "Defaced image saved as:\n",
      "  sub-017_T1w_defaced.nii\n",
      "Cleaning up...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# os.chdir(bids_data_dir)\n",
    "# all_subs = next(os.walk(org_data_dir))[1]\n",
    "# i = 0\n",
    "# anat_dir = os.path.join(bids_data_dir, all_subs[0], 'anat')\n",
    "# os.chdir(anat_dir)\n",
    "# t1w = glob.glob('*T1w*')\n",
    "# t1w_fn = t1w[0]\n",
    "# !cd $anat_dir && pydeface $t1w_fn\n",
    "\n",
    "\n",
    "#sub 17 already defaced\n",
    "\n",
    "\n",
    "# for path, subdirs, files in os.walk(anat_dir):\n",
    "#     for name in files:\n",
    "#         print(os.path.join(path, name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: Convert Eprime data to BIDS\n",
    "\n",
    "Use `neufepme_read_eprime.ipynb`, which uses `convert-eprime`\n",
    "\n",
    "Perhaps write this into separate functions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/05-task-events.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.4: Convert physiology data to BIDS\n",
    "\n",
    "Using `scanphyslog2bids`\n",
    "\n",
    "See: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/06-physiological-and-other-continuous-recordings.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-09 14:04:33,301 [MainThread] [INFO   ]  Found end marker ('0020') at an offset of 32 samples relative to the end of the file.\n",
      "2020-01-09 14:04:37,376 [MainThread] [INFO   ]  Trimmed off 59 samples from end of file based on the (absence) of a gradient.\n"
     ]
    },
    {
     "ename": "CouldNotFindThresholdError",
     "evalue": "Could not find threshold!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCouldNotFindThresholdError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-729db9a9ff63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# (either \"vol_markers\", \"gradient_log\", or \"interpolation\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# phlog.align(trigger_method='vol_markers')  # load and find vol triggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mphlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigger_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gradient_log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load and find vol triggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Write out BIDS files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rtme-fmri/lib/python3.7/site-packages/scanphyslog2bids/core.py\u001b[0m in \u001b[0;36malign\u001b[0;34m(self, trigger_method, which_grad, trigger_diff_cutoff, offset_end_scan)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# Do the actual hard work below (at least, for the gradient method)!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrigger_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'gradient_log'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_triggers_by_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtrigger_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'interpolate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_triggers_by_interpolation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset_end_scan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rtme-fmri/lib/python3.7/site-packages/scanphyslog2bids/core.py\u001b[0m in \u001b[0;36m_determine_triggers_by_gradient\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mthr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mthr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# shit, didn't find it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mCouldNotFindThresholdError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not find threshold!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_triggers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_triggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCouldNotFindThresholdError\u001b[0m: Could not find threshold!"
     ]
    }
   ],
   "source": [
    "from scanphyslog2bids.core import PhilipsPhysioLog\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "main_dir = '/Users/jheunis/Desktop/sample-data/sub-neufepmetest/sub-pilot/raw-data'\n",
    "log_file = main_dir + '/SCANPHYSLOG20191011115018_motor1.log'\n",
    "out_dir = main_dir + '/my_bids_data'  # where the BIDSified data should be saved\n",
    "deriv_dir = main_dir + '/my_bids_data/physio'  # where some QC plots should be saved\n",
    "\n",
    "# fmri_file is used to extract metadata, such as TR and number of volumes\n",
    "fmri_file = main_dir + '/sub-pilot_task-motor_run-1_echo-2.nii' \n",
    "fmri_img = nib.load(fmri_file)\n",
    "n_dyns = fmri_img.shape[-1]\n",
    "tr = np.round(fmri_img.header['pixdim'][4], 3)\n",
    "\n",
    "# Create PhilipsPhysioLog object with info\n",
    "phlog = PhilipsPhysioLog(f=log_file, tr=tr, n_dyns=n_dyns, sf=500, manually_stopped=False)\n",
    "\n",
    "# Load in data, do some preprocessing\n",
    "phlog.load()\n",
    "\n",
    "# Try to align physio data with scan data, using a particular method\n",
    "# (either \"vol_markers\", \"gradient_log\", or \"interpolation\")\n",
    "# phlog.align(trigger_method='vol_markers')  # load and find vol triggers\n",
    "phlog.align(trigger_method='interpolation')  # load and find vol triggers\n",
    "\n",
    "# Write out BIDS files\n",
    "phlog.to_bids(out_dir)  # writes out .tsv.gz and .json files\n",
    "\n",
    "# Optional: plot some QC graphs for alignment and actual traces\n",
    "phlog.plot_alignment(out_dir=deriv_dir)  # plots alignment with gradient\n",
    "phlog.plot_traces(out_dir=deriv_dir)  # plots cardiac/resp traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run BIDS validator\n",
    "\n",
    "see: https://bids-standard.github.io/bids-validator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run scripts to ensure that BIDS json data \n",
    "TR = 2\n",
    "N_slices = 34\n",
    "t_d = TR/N_slices\n",
    "t_start = 0\n",
    "t_stop = TR\n",
    "slice_timing = np.arange(start=t_start, stop=t_stop, step=t_d).round(4).tolist()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if possible to run validator on a full directory tree\n",
    "\n",
    "# fn = '/Users/jheunis/Documents/PYTHON/rtme-fMRI/bidsify_test_2709/sub-01/anat/sub-01_T1w.nii'\n",
    "# BIDSValidator().is_bids(fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create summary tables and plots using `pybids`\n",
    "\n",
    "See:\n",
    "- https://bids-standard.github.io/pybids/\n",
    "- https://github.com/bids-standard/pybids/blob/master/examples/pybids_tutorial.ipynb\n",
    "- https://mybinder.org/v2/gh/bids-standard/pybids/master\n",
    "\n",
    "this is also useful for de/constructing filenames, might be useful in earlier steps of this workflow already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
